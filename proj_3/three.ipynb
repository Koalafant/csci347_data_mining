{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 347 - Project 3 </br>\n",
    "#### Mason Reyher, Colby Roberts, Charlie Weitzenberg, Erin Scheunemann, Megan Fehres"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a data set that you are interested in from the UCI Machine Learning Repository that\n",
    "has at least 5 numerical attributes, and that you believe may contain clusters. Only use the\n",
    "numerical attributes for this project. Note: if you are planning to complete the extra credit\n",
    "portion of this project, you will need to use a data set that has class labels (ground truth\n",
    "cluster labels), i.e., a classification data set, in order to compute the accuracy of the clustering.\n",
    "If you would like to use a data set from a different source, please discuss this with me.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Think about the data\n",
    "Answer the following questions:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tetuan city power consumption dataset was chosen. </br>\n",
    "52417 rows, 9 attributes. 8 numerical attributes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [1 point] Why are you interested in this data set?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know next to nothing about Tetouan, a city in Morocco. However, being in the northern tip of Africa (and a maritime climate), I presume the climate of this area is dynamic to say the least. I imagine there will be clusters based on the temperatures and certain city zones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [1 point] How many numerical attributes and categorical attributes are there in the data set?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8 numerical attributes and 1 categorical (datetime)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [1 point] Are there any missing values? If there are missing values, how are you planning to\n",
    "handle these (will all data instances with missing values be removed? Will all attributes with\n",
    "missing values be removed? Will missing values be imputed? If so, how?)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Before doing any analysis, answer the following questions:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [1 point] Why do you expect clusters to be present in the data?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When temperature is drastically high, many in the city will turn on their air conditioning (high power usage). There should be a postive correlation and cluster (using DBSCAN, not kmeans) as temperature increases, with power usage. It will be interesting to see if the city zone will cluster separately -- a rich district versus a poor district. However, due soley to temperature and energy usage, there will be some sort of cluster(s)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [1 point] Why might finding clusters in this data set be helpful (how might this help us\n",
    "understand or analyze the data)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are clusters present between different city zones, we can use that to determine what district of the city is using tons of power. In an effort to save power, or an initiative to reduce power usage (plant trees on the tops of buildings to absorb heat in the high usage areas) we can use clustering to gain insight in how to reduce power usage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [1 point] How many clusters do you expect to see in the data? Provide a range of values\n",
    "to answer this question. For example, 2 to 4. Why do you expect a number of clusters\n",
    "in this range?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between 2 and 3. Either there will be no cluster between city districts, and there will only be clusters correlating to power usage and temperature; or there will be three clusters between power usage and city districts. If the latter clusters are true, there will be much more useful data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [1 point] Do you expect that the clusters will be of similar size (i.e., cluster 1 is about the\n",
    "same size as cluster 2, is about the same size as cluster 3, etc..)? Why or why not?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They probably won't be. The city districts are probably not the same size. More people live in one area but are poorer, and less people live in another and have more money. For example, in a richer district with less people, they'll use more power for air conditioning in the summer. In a poorer district, there will be more people, but cannot afford to use air conditioning as much, therefore power usage will decrease (this could be the complete opposite too). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Write functions for graph analysis in Python\n",
    "Write the following functions in Python. You may use scikit-learn or other packages to check\n",
    "the correctness of your implementation, but you may not use any existing clustering algorithm\n",
    "implementation in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas -q\n",
    "%pip install numpy -q\n",
    "%pip install scikit-learn -q\n",
    "%pip install matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data = pd.read_csv('tetuan_city_power_consumption.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [10 points] A function that implements the k-means clustering algorithm. The function\n",
    "should take a data matrix, a number of clusters k, and a convergence parameter epsilon,\n",
    "as input, and return the representatives (means) as well as the clusters found using k-\n",
    "means. If the distance is the same between a point and more than one representative\n",
    "(mean), then assign the point to the mean corresponding to the cluster with the lowest\n",
    "index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as m\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def kmeans(data, k, epsilon):  # epsilon is the amount of change acceptable\n",
    "    dimension = len(data[0])\n",
    "    max = np.amax(data, 0) # max vals in each col\n",
    "    min = np.amin(data,0) # min vals in each col\n",
    "    guesses = [[0 for i in range(0,dimension)] for j in range(0,k)] # array for guesses with dimension, dimension x k\n",
    "    for i in range(0, k):\n",
    "        for j in range(0, dimension):\n",
    "            guesses[i][j] = random.uniform(min[j], max[j]) # random num between min and max for col\n",
    "    return kcluster(data, guesses, epsilon) # input data, initial randomized means, and epsilon\n",
    "\n",
    "def kcluster(data, means, epsilon): # this is so we can go again until difference in means < epsilon\n",
    "    cluster_assignments = np.zeros(len(data))  # make all assignments 0\n",
    "    distances = np.ones(len(data))*np.inf # set all dist to infinity\n",
    "    num_nodes_in_cluster = [0 for i in range(0, len(means))]\n",
    "    for point in range(0, len(data)): # this sorts the points into the k clusters based on dist to means\n",
    "        for k in range(0, len(means)):\n",
    "            new_distance = m.dist(data[point], means[k]) # dist from point to mean\n",
    "            if new_distance < distances[point]: # if the distance is smaller (THIS IS WHY WE START WITH INF)\n",
    "                distances[point] = new_distance # set to smaller dist\n",
    "                cluster_assignments[point] = k # assign to that mean's cluster\n",
    "        num_nodes_in_cluster[int(cluster_assignments[point])] += 1 # add one to total nodes in cluster\n",
    "    new_means = [[0 for i in range(0, len(data[0]))] for j in range(0, len(means))]\n",
    "    for point in range(0, len(data)): # add up all points in the clusters\n",
    "        new_means[int(cluster_assignments[point])] = np.add(new_means[int(cluster_assignments[point])], data[point])\n",
    "    for k in range(0, len(means)): # divide all sums by the total num in that cluster\n",
    "        new_means[k] = np.divide(new_means[k], num_nodes_in_cluster[k])\n",
    "    for k in range(0, len(means)):\n",
    "        if m.dist(means[k], new_means[k]) > epsilon: # if the dist greater than epsilon\n",
    "            kcluster(data, new_means, epsilon) # run again\n",
    "    return new_means, cluster_assignments # otherwise if all dist < epsilon return means and assignments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. [10 points] A function that implements the DBSCAN clustering algorithm. The function\n",
    "should take a data matrix and the parameters minpts and epsilon as input, and return the\n",
    "clusters found using DBSCAN, where each data point is labeled as either a noise point, a\n",
    "border point, or a core point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan(df, min, e):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Extra credit (5 points): A function that computes the precision of a clustering. The function\n",
    "should take a list of true cluster labels and a list of the cluster labels returned by some\n",
    "clustering algorithm, and return the precision of the clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_prec(true_labels, labels):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Analyze your data\n",
    "Report the following, using tables or figures as appropriate. You may use scikit-learn’s\n",
    "implementation of k-means and DBSCAN, but you are encouraged to first try using your own\n",
    "implementations on real-world data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. [4 points] Use sklearn’s PCA implementation to linearly transform the data to two\n",
    "dimensions. Create a scatter plot of the data, with the x-axis corresponding to coordinates\n",
    "of the data along the first principal component, and the y-axis corresponding to\n",
    "coordinates of the data along the second principal component. Does it look like there are\n",
    "clusters in these two dimensions? If so, how many would you say there are?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. [3 points] Use sklearn’s PCA implementation to linearly transform the data, without\n",
    "specifying the number of components to use. Create a plot with r, the number of\n",
    "components (i.e., dimensionality), on the x-axis, and f(r), the fraction of total variance\n",
    "captured in the first r principal components, on the y-axis. Based on this plot, choose a\n",
    "number of principal components to reduce the dimensionality of the data. Report how\n",
    "many principal components will be used as well as the faction of total variance captured\n",
    "using this many components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '1/1/2017 0:00'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m r \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m      4\u001b[0m pca \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39mdecomposition\u001b[39m.\u001b[39mPCA(n_components \u001b[39m=\u001b[39m r)\n\u001b[1;32m----> 5\u001b[0m tf \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39;49mfit_transform(city_data)\n\u001b[0;32m      6\u001b[0m varExp \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39mexplained_variance_ratio_\n\u001b[0;32m      7\u001b[0m cs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcumsum(varExp)\n",
      "File \u001b[1;32mc:\\Users\\Colby\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Colby\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:462\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \n\u001b[0;32m    441\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mC-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m--> 462\u001b[0m U, S, Vt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[0;32m    463\u001b[0m U \u001b[39m=\u001b[39m U[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components_]\n\u001b[0;32m    465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhiten:\n\u001b[0;32m    466\u001b[0m     \u001b[39m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Colby\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_pca.py:485\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[0;32m    480\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPCA does not support sparse input. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTruncatedSVD for a possible alternative.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n\u001b[1;32m--> 485\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    486\u001b[0m     X, dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32], ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[0;32m    487\u001b[0m )\n\u001b[0;32m    489\u001b[0m \u001b[39m# Handle n_components==None\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Colby\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:546\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 546\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    547\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    548\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\Colby\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Colby\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Colby\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:2070\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2069\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2070\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '1/1/2017 0:00'"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing\n",
    "import sklearn.decomposition\n",
    "r = 2\n",
    "pca = sklearn.decomposition.PCA(n_components = r)\n",
    "tf = pca.fit_transform(city_data)\n",
    "varExp = pca.explained_variance_ratio_\n",
    "cs = np.cumsum(varExp)\n",
    "\n",
    "plt.plot(range(1, r+1), cs)\n",
    "plt.xlabel('Number Of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by Principal Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. [5 points] For both the original and the reduced-dimensionality data obtained using PCA in\n",
    "question 9, do the following: Experiment with a range of values for the number of clusters,\n",
    "k, that you pass as input to the k-means function, to find clusters in the chosen data set.\n",
    "Use at least 5 different values of k. For each value of k, report the value of the objective\n",
    "function for that choice of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. [5 points] For both the original and the reduced-dimensionality data obtained using PCA in\n",
    "question 9, do the following: Experiment with a range of values for the mints and epsilon\n",
    "input parameters to the DBSCAN function to find clusters in the chosen data set. First keep\n",
    "epsilon fixed and try out a range of different values for minpts. Then keep minpts fixed, and\n",
    "try a range of values for epsilon. Use at least 5 values of epsilon and at least 5 values of\n",
    "minpts. Report the number of clusters found for each (minpts, epsilon) pair tested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Extra credit (3 points): Create a plot of clustering precision for each value of k used in\n",
    "question 10, each value of epsilon used in question 11, and each value of mints used in\n",
    "question 11, for both the original and reduced-dimensionality data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
